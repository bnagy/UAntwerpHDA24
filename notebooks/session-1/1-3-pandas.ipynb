{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Dataframes\n",
    "\n",
    "In previous years we ran this course using R, however this year we will continue to work with Python. As you have learned, the most common library for manipulating structured data in Python is called `pandas`. This notebook covers a quick refresher and tour, using some of the data we will work with later in this course.\n",
    "\n",
    "There are a lot of pandas resources available, eg [cheatsheets](https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf).\n",
    "\n",
    "The data that we have just loaded comes from the file containing the Arthurian manuscripts metadata. Each row represents a single manuscript and the columns describe different properties of each of these books (or at least, what remains of them).\n",
    "\n",
    "First, let's load the data from a CSV file on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../datasets/arthur/manuscripts.csv\", index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `shape` property to find the dimensions of the `DataFrame` object (rows x columns). Remember that Python *properties* are accessed as bare words (`df.shape` not `df.shape()`), whereas *methods* must have parentheses, even when no arguments are given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other useful properties. We can look at the `columns`, and later we will make a lot of use of the `index`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Methods\n",
    "\n",
    "We can take a quick look at the DataFrame using the `head()` and `tail()` methods, but also note how Jupyter notebooks will already return a basic view of the DataFrame as long as it is the last thing evaluated in a cell, so mostly we will use that 'shortcut' below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the last object evaluated in a cell is outputted by Jupyter\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we access an individual column we will get a `Series`. This can also be done with a magic method that treats the column name like a property. The two calls below are mostly equivalent (sometimes the longer string subscript access is required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"script\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most methods on a DataFrame will return a new DataFrame. Here we pull out the width and height of the pages (leaves) for those manuscripts where the data is recorded. Since that method returns a `DataFrame` we can immediately *chain* another method, `dropna()`, to drop all rows where either value is NA.\n",
    "\n",
    "NOTE CAREFULLY: The indices for the rows *do not change* (unless we force it to by using a different method). That means our indices are no longer sequential. This is an advantage!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = df[[\"leaf-height\", \"leaf-width\"]].dropna()\n",
    "dimensions.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can pull the rows from the original dataframe, but ONLY at the index locations (`.loc`) specified in the `dimensions` dataframe. Note how we now have a dataframe with only 727 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[dimensions.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operating on Columns\n",
    "\n",
    "We can insert new `Series` objects (or actually even normal lists, or numpy arrays, etc) into the dataframe as columns. Here, we create one column using a vectorized boolean comparison, and another one which we create by multiplying two existing columns. Once again note how these are *vector* operations -- `ColA * ColB` multiplies the matching values in every row, all in one statement.\n",
    "\n",
    "Note how in the automatic view the reported shape is increasing by one column each time...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert takes index-to-insert, column-name, Series as positional arguments\n",
    "\n",
    "df.insert(3, \"illustrated\", df.illustrations > 0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.insert(4, \"surface\", df[\"leaf-height\"] * df[\"leaf-width\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just Checking...\n",
    "\n",
    "Let's just see if the first entry by *position* (not index, `iloc` always uses sequential position) has been correctly calculated. This is not exhaustive, but it is a good habit to make sure that things look like they are doing what they should be doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "308 * 215 == df.iloc[0][\"surface\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More index tricks\n",
    "\n",
    "Now we'll save the new dataframe we get by accessing the original `df` at the indices from the `dimensions` df, into a new variable called `df_valid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = df.loc[dimensions.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorting is easy in pandas, using `sort_values` (this sorts the whole row, using the values in the column you provide)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.sort_values(by=\"surface\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... but notice that some of `surface` entries are 0, because one of `leaf-height` or `leaf-width` was mistakenly entered as 0 instead of NA. Let's clean that up.\n",
    "\n",
    "The main tool for subsetting by conditions inherently uses a boolean Series under the hood, but you can think of it like this: \"return the rows of `df_valid` at the indices where `df_valid.surface` (a column) is greater than zero\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = df_valid[df_valid.surface > 0]\n",
    "df_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping and aggregating\n",
    "\n",
    "A very powerful method is pandas `groupby()` which creates a special grouping object that can be used for a lot of things. For example, let's see how many illustrated manuscripts occur for each script type..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.groupby(\"script\")[\"illustrated\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also \"normalize\" these numbers by count to obtain comparable proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.groupby(\"script\")[\"illustrated\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, and this is just scratching the surface, we can \"aggregate\" columns from the groups using any function. Common functions (mean, median, std...) can be specific just by their name..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid.groupby(\"script\")[\"surface\"].agg(\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can subset our valid dataframe again by directly using the boolean column `illustrated` that we created earlier. Now we're down to just 186 manuscripts where we have illustrations as well as valid page sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_illustrated = df_valid[df_valid.illustrated]\n",
    "df_illustrated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other formats\n",
    "\n",
    "Finally, we use pandas again to read the same data from Microsoft Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import openpyxl\n",
    "except ImportError:\n",
    "    !pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfxl = pd.read_excel(\"../../datasets/arthur/manuscripts.xlsx\")\n",
    "dfxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no! The Excel indices start from 0, but the CSV we used before started from 1 (maybe it was created in R or Matlab). We can just use our vector operation trick again to add one to the entire index column (ie to each row in it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfxl.index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now (with the indices matching), we use the index locations from our `df_illustrated` dataframe but we grab the rows straight from the new Excel dataframe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfxl.loc[df_illustrated.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, a little taste of validation and cleaning. We can use `.equals()` to compare entire `Series` or `DataFrame` objects. Here we find out that the `signature` columns are *not* equal, even though they should be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfxl.loc[df_illustrated.index][\"signature\"].equals(df_illustrated[\"signature\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A different method, `.eq()`, returns a boolean `Series` instead (with an equality value for every row). By using a sneaky trick, we assign that to a variable, and then invert it using logical NOT (the tilde `~` operator in pandas). In other words, the original series was `True` almost everywhere and `False` in a couple of places, we swap that, so it is `True` just for the problem rows, and see what indices they are.\n",
    "\n",
    "Take your time with this idea, it is not simple to understand at first. It can help to make new cells, and then look at individual variables. What is `s`? What does `~s` look like? etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = dfxl.loc[df_illustrated.index][\"signature\"].eq(df_illustrated[\"signature\"])\n",
    "df_illustrated[~s]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and let's take a look. Oh, this old trick -- someone has accidentally left an extra space at the end of two column entries. This kind of problem comes up all the time in data analysis, and requires constant vigilance. Fixing these problems is called \"data cleaning\" and,unfortunately, is a necessary process for almost all real-world work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfxl.loc[308][\"signature\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_illustrated.loc[308][\"signature\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to finish, a little trick with Python \"f-strings\". It is not as complicated as it looks, see if you can figure it out!\n",
    "\n",
    "(Again, try breaking it into smaller pieces, and then putting them back together!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"A: {repr(dfxl.loc[596]['signature'])}\\nB: {repr(df_illustrated.loc[596]['signature'])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Version History\n",
    "\n",
    "Current: v1.0.3\n",
    "\n",
    "10/9/24: 1.0.0: first draft, BN\n",
    "18/9/24: 1.0.1: add group_by examples, BN\n",
    "04/10/24: 1.0.2: correct typos and proofread, MK\n",
    "13/10/24: 1.0.3: move to public, BN\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
